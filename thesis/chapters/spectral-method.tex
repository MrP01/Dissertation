\chapter{Spectral Method}
\label{chap:spectral-method}

% \section{Content}
% \input{chapters/out/Spectral Method.md.tex}

In this chapter we will construct a spectral method in the basis of Jacobi polynomials to explore the solution of equilibrium distributions $\hat{\rho}(\hatvec{x})$.
Starting from a many-body system and considering the continuous limit as $N_p \goesto \infty$, in \Cref{chap:particle-interaction-theory} we have already established the governing equation of the particle density distribution $\hat{\rho}(\hatvec{x})$ in such a system.

As mentioned in \Cref{chap:particle-interaction-theory}, the numerical approaches will be carried out on the normalised domain $B_1(\vec{0})$ and we will be looking for $\rho \in \functionspace$, whilst in general we are interested in the $\hat{\rho} \in \functionspacehat$ solving \href{def:the-problem}{our problem}.
Both versions are related by $\hat{\rho}(\hatvec{x}) = \rho(\vec{x})$ and $\hatvec{x} := R \vec{x}$.

Can we put together a numerical method to solve for the equilibrium distribution (cf. \Cref{def:equilibrium-measure})?
Let us consider the problem from the bottom up and start from the solution:
The basic idea behind spectral methods is to assume a solution $\rho(\vec{x})$ of the form
$$\rho(\vec{x}) = \sum_{k=0}^{N-1} \rho_k \varphi_k(\vec{x})\,,\quad \rho_k \in \R, \varphi_k: \R^d \mapsto \R\,,\quad k = 0, ..., N-1\,,$$
with $N$ coefficients $\vec{\rho} := \left(\rho_0, ..., \rho_{N-1}\right)^T$ multiplying $N$ basis functions $\varphi_k$.

\pagebreak
\section{Special Functions}
The following section will introduce a few necessary objects and tools to understand the basis of functions we are working with to construct the spectral method, the basis of Jacobi polynomials.

We start with the Pochhammer symbol, another name for the \textit{rising factorial}, an unusual notation for a function but standard in the context of the special functions that will be introduced on top of it.
\input{chapters/bits/rising-factorial.tex}

As a second prerequisite, we introduce the closely intertwined beta- and gamma-functions (\Cref{def:beta-function}, \Cref{def:gamma-function}).
\input{chapters/bits/gamma-function.tex}
\input{chapters/bits/beta-function.tex}

Using the Pochhammer symbol introduced in \Cref{def:rising-factorial}, we can now define the generalised hypergeometric series ${}_pF_q$ (cf. \Cref{def:generalised-hypergeometric-series}) and a special case of it, the Gaussian hypergeometric function (cf. \Cref{lemma:gaussian-hypergeometric-function}).
\input{chapters/bits/generalised-hypergeometric-series.tex}
\input{chapters/bits/gaussian-hypergeometric-function.tex}

\pagebreak
\section{Orthogonal Polynomials forming a Basis}
In order to efficiently construct a spectral method, we need an orthogonal basis.
\input{chapters/bits/orthogonal-polynomials.tex}
\input{chapters/bits/three-term-recurrence-relationship.tex}

The Jacobi polynomials are then defined from ${}_2F_1$ as follows:
\input{chapters/bits/jacobi-polynomials.tex}
% \input{chapters/bits/gegenbauer-polynomials.tex}
% \input{chapters/bits/chebyshev-polynomials.tex}
\input{chapters/bits/jacobi-matrix.tex}

% \pagebreak
\section{Working towards a Solution}
Finally, now that we have established the basis functions, we can write down an ansatz $\rho: B_1(\vec{0}) \mapsto \R$ for the solution of \hyperref[def:the-problem]{the problem}, of the form
\begin{equation}
  \rho(\vec{x}) := \left(1-\norm{\vec{x}}^{2}\right)^{m - \frac{\alpha + d}{2}} \sum_{k=0}^{N-1} \rho_k P_{k}^{\left(m - \frac{\alpha + d}{2},\frac{d-2}{2}\right)}(2 \norm{\vec{x}}^{2}-1)\,.
  \label{eq:ansatz}
\end{equation}
with $P_k^{(a, b)}$ the Jacobi polynomials and $\{\rho_k\}_{k=0, ..., N-1}$ the coefficients.
% TODO: is it alpha or beta in the exponent of (1-y\^{}2)?

The spectral method can then be written as a linear system of the coefficients $\vec{\rho}$ as we will see in the next section.
In order to establish said linear system, we first need to introduce the \textit{inverse fractional Laplacian}, helping us with the evaluation of the power-law potential integral involving radial Jacobi polynomials given in \Cref{thm:theorem216}, the most important result of this chapter.

\input{chapters/bits/riesz-potential.tex}
\input{chapters/bits/theorem216.tex}

% \Cref{thm:theorem216} gives an explicit expression for the main integral \(\hat{Q}^{\beta}: L \mapsto L\), an operator from the function space \(L\) to the function space \(L\), we are interested in:
% which is used to construct the spectral method operator \(Q^\beta\) (cf. \Cref{def:operator}), acting on the coefficients \(\vec{\rho}\).

Adding to our collection of tools, in order to solve the problem given in \Cref{def:the-problem} we need to normalise the solution by its mass.
The normalisation constant is given in \Cref{lemma:mass}, based only on a single coefficient $\rho_0$, allowing for a highly efficient renormalisation.
\input{chapters/bits/mass-of-solution.tex}

\pagebreak
\section{Derivation of the Operator}
\input{chapters/bits/operator.tex}
\input{chapters/bits/derivation-of-operator-matrix.tex}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{results/attrep/attractive-repulsive-operators.pdf}
  \caption[Attractive and repulsive operators.]{The attractive and repulsive operators (matrices) as given in \Cref{def:operator}, the matrix values are shown in a $\log_{10}$ color scale. Due to the choice of basis, the attractive operator is exactly banded. The repulsive parameter is only approximately banded, which the spy plots effectively demonstrate.}
  \label{fig:attractive-repulsive}
\end{figure}

The bandedness of the attractive operator in \Cref{fig:attractive-repulsive} is due to the three-term recurrence relationship of the Jacobi polynomial basis (cf. \Cref{thm:three-term-recurrence-relationship} and \Cref{def:jacobi-polynomials}).
% TODO.. explain

For the attractive-repulsive interaction potential $K_{\alpha,\beta}(r)$, the full operator is given by
\begin{align*}
  \hat{Q}_{\alpha, \beta}[\hat{\rho}](\hatvec{x}) & := \int_{B_R(\vec{0})} K_{\alpha,\beta}\left(\norm{\hatvec{x} - \hatvec{y}}\right) \hat{\rho}(\hatvec{y}) \,\dd\hatvec{y}                                                               \\
                                                  & = \int_{B_R(\vec{0})} \left(\frac{\norm{\hatvec{x} - \hatvec{y}}^\alpha}{\alpha} - \frac{\norm{\hatvec{x} - \hatvec{y}}^\beta}{\beta}\right) \hat{\rho}(\hatvec{y}) \,\dd\hatvec{y}     \\
                                                  & = \int_{B_1(\vec{0})} \left(\frac{R^\alpha \norm{\vec{x} - \vec{y}}^\alpha}{\alpha} - \frac{R^\beta \norm{\vec{x} - \vec{y}}^\beta}{\beta}\right) \hat{\rho}(R\vec{y}) \, R^d\dd\vec{y} \\
                                                  & = R^d\int_{B_1(\vec{0})} \left(\frac{R^\alpha}{\alpha}\norm{\vec{x} - \vec{y}}^\alpha - \frac{R^\beta}{\beta} \norm{\vec{x} - \vec{y}}^\beta\right) \rho(\vec{y}) \,\dd\vec{y}          \\
                                                  & = \frac{R^{\alpha+d}}{\alpha} \hat{Q}^\alpha[\rho](\vec{x}) - \frac{R^{\beta+d}}{\beta} \hat{Q}^\beta[\rho](\vec{x})\,,
\end{align*}
where one needs to carefully handle the variable transform with $\dd\hatvec{y} = R^d \dd\vec{y}$ in $d$ dimensions whereas the vectors themselves obey $\hatvec{y} = R \vec{y}$ as established previously.
In matrix form that is,
\begin{equation}
  Q_{\alpha, \beta} := \frac{R^{\alpha+d}}{\alpha} Q^\alpha - \frac{R^{\beta+d}}{\beta} Q^\beta
  \label{eq:full-attrep-operator}
\end{equation}
for some interval radius $R \in \R^+$, usually chosen as the smallest possible $R$ such that $\supp(\rho) \subseteq [-R, R]$.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{results/attrep/full-operator.pdf}
  \caption[Combination of the attractive-repulsive operators]{Spy plot of $Q_{\alpha, \beta}$, the combination of the attractive-repulsive operators. Inverting this operator and applying it to $(1, 0, ..., 0)^T \in \R^N$ will yield the unnormalised coefficients $\rho_k$ of the solution expansion given in \Cref{eq:ansatz}.}
  \label{fig:attrep-operator}
\end{figure}

\pagebreak
\section{Solving a Linear System}
Once the operator is computed, we are now looking for a set of solution coefficients $\vec{\rho} \in \R^N$ such that the energy on the domain $D = B_R(\vec{0})$ is constant.
That means, we are looking for $\vec{\rho} \in \R^N$ such that
\begin{equation}
  \hat{Q}[\rho](\vec{x}) = \tilde{E}(\vec{x}) = E\,,
\end{equation}
where we can expand $\tilde{E}(\vec{x}) = \jacobivec{x} \cdot \vec{E}$ into Jacobi polynomials with coefficients $\vec{E} = (E, 0, ..., 0)^T$ such that the energy is constant along the entire domain, so $\tilde{E}(\vec{x}) = E \cdot P_0^{(a, b)}\left(\jacobiarg{x}\right) = E$.
In matrix form, that is
$$Q \vec{\rho} = \vec{E} \qLRq \begin{pmatrix}
    q_{00}    & \dots  & q_{0,N-1}   \\
    \vdots    & \ddots & \vdots      \\
    q_{N-1,0} & \dots  & q_{N-1,N-1} \\
  \end{pmatrix} \begin{pmatrix}
    \rho_0 \\
    \vdots \\
    \rho_{N-1}
  \end{pmatrix} = \begin{pmatrix}
    E       \\
    \vec{0} \\
    0
  \end{pmatrix}\,.$$

\section{Results}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{results/attrep/solution-increasing-order.pdf}
  \caption[Solutions of increasing orders]{Particle density distribution function solutions $\rho$ of increasing order $N$ to the attractive-repulsive problem with interaction potential $K_{alpha, \beta}(r)$, $\alpha = 2.5$ and $\beta = 1.2$. Reflected along the y-axis for better visibility of the domain.}
  \label{fig:solution-increasing-order}
\end{figure}

\section{Outer Optimisation Routine}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{results/known-analytic/outer-optimisation.pdf}
  \caption[Outer Optimisation Routine]{The total potential $U$ as a function of the support radius $R$. This is the goal function minimised by the outer optimisation routine.}
  \label{fig:outer-optimisation}
\end{figure}

Note that using this setup, the operators themselves do not need to be recomputed upon a change in $R$ (cf. \Cref{eq:full-attrep-operator}).
The provided implementation uses \gls{lru} caching to automatically store operators for a given parameter set and order $N$.

\section{Comparison with Analytic Solutions}
As introduced in \Cref{sec:analytical-solutions}, there are some analytical solutions available which allow us to perform some further analysis of the numerical method in these special cases.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{results/known-analytic/analytic-solution.pdf}
  \caption[Comparison with analytical solutions and error]{
    The analytic solution $\rho(x)$ given in \Cref{eq:analytical-solution-alpha-equal-2} compared to the (spectral method) solutions of different order $N$.
    The ``arches'' occur as a result of the roots of $\rho(x) - \rho_N(x)$, their number approximately equals the order $N$ (a polynomial of degree $N$ has $N$ roots).
  }
  \label{fig:analytic-solution}
\end{figure}

There are more analytic solutions available for other parameter ranges, we will not analyse them here.

\input{chapters/bits/spectral-convergence.tex}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{results/known-analytic/convergence-to-analytic.pdf}
  \caption[Convergence to analytic solution]{Convergence of the numerical solution to the known analytic solution (cf. \Cref{eq:analytical-solution-alpha-equal-2}) in a special case where it is known, squared error plotted as a function of the highest order in the expansion $N$.}
  \label{fig:convergence-to-analytic}
\end{figure}

\section{Discussion}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{results/attrep/convergence.pdf}
  \caption[Step-by-step convergence of solutions compared to order 24]{Step-by-step convergence of numerical solutions $\rho_N(x)$ as compared to $\rho_{24}(x)$, visualised using the squared error of the pointwise evaluation of both functions in $200$ points.}
  \label{fig:convergence}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{results/attrep/varying-parameters.pdf}
  \caption[Varying parameters in the solver]{
    Varying different parameters in the solver to demonstrate their effect.
    See also, \Cref{fig:varying-R-solutions}.
  }
  \label{fig:varying-parameters}
\end{figure}
